---
title: "Emotional Attributes of diverging political spheres on Twitter"
author: "Florian Klement, David Siegl, Shirin Yanni"
output:
  html_document:
    df_print: paged
  pdf_document: default
---


```{r}
library(rtweet)
library(ggplot2)
library(tidyverse)
library(gsubfn)
library(quanteda)
library(tm)
quanteda::stopwords
```

<b>Accessing the Twitter-API</b>
```{r}
consumerKey = "X"
consumerSecret = "X"
accessToken = "X-X"
accessSecret = "X"
options(httr_oauth_cache=TRUE)
twitter_token <- create_token(
  consumer_key = consumerKey,
  consumer_secret = consumerSecret,
  access_token = accessToken,
  access_secret = accessSecret)
```


<b>Loading some data</b>
```{r}
sheet <- read.csv("./Emotion Analysis - Twitter.csv")
id_list <- as.list(sheet$Twitter.ID)
```


<b>Defining Function for Twitter-Scraping</b>
```{r}
scrape_tweets <- function(userID, n_tweets, filename){
  tweets <- get_timeline(userID, n_tweets, include_rts=FALSE, exclude_replies=TRUE)
  tweets_df <- as_data_frame(tweets)
  tweets_df <- tweets_df %>%
    select(c(status_id, text, screen_name, created_at, favorite_count, retweet_count))
  write.csv(tweets_df, paste(filename,"-tweets.csv", sep = ""), row.names = FALSE)
}

# for (id in id_list) {
#   scrape_tweets(id, 3200, id)
# }
```


<b>Loading the Tweets</b>
```{r}
tweets_files <- list.files("./data/raw")

for (i in 1:length(tweets_files)) {                              
  assign(paste0("tweets_", i),                                   
         read.csv(paste0("./data/raw/",
                   tweets_files[i])))
}
```


<b>Preprocessing</b>
```{r}
tweets_1 <- top_n(tweets_1, 300, created_at) #sampling the tweets for n=300
tweets_2 <- top_n(tweets_2, 300, created_at)
tweets_4 <- top_n(tweets_4, 300, created_at)
tweets_5 <- top_n(tweets_5, 300, created_at)
tweets_6 <- top_n(tweets_6, 300, created_at)
tweets_7 <- top_n(tweets_7, 300, created_at)
tweets_8 <- top_n(tweets_8, 300, created_at)
tweets_9 <- top_n(tweets_9, 300, created_at)
tweets_10 <- top_n(tweets_10, 300, created_at)
tweets_11 <- top_n(tweets_11, 300, created_at)
tweets_12 <- top_n(tweets_12, 300, created_at)
tweets_13 <- top_n(tweets_13, 300, created_at)
tweets_14 <- top_n(tweets_14, 300, created_at)
tweets_15 <- top_n(tweets_15, 300, created_at)
tweets_16 <- top_n(tweets_16, 300, created_at)
```


```{r}
cleaning_tweets <- function(text){
  text <- gsub("http.+", "", text)
  text <- gsub(",", "", text)
  text <- gsubfn(pattern = "[[:punct:]]", engine = "R",
       replacement = function(x) ifelse(x == "#", "#", ""), 
       text)
  text <- gsub("[[:digit:]]", "", text)
  text <- gsub("  ", " ", text)
  text <- tolower(text)
}
```


```{r}
list <- list(tweets_1, tweets_2, tweets_3, tweets_4, tweets_5, tweets_6, tweets_7, tweets_8, tweets_9, tweets_10, tweets_11, tweets_12, tweets_13, tweets_14, tweets_15, tweets_16)
df_full <- rbind(tweets_1, tweets_2, tweets_3, tweets_4, tweets_5, tweets_6, tweets_7, tweets_8, tweets_9, tweets_10, tweets_11, tweets_12, tweets_13, tweets_14, tweets_15, tweets_16, by = (c("status_id", "text", "screen_name", "created_at", "favorite_count", "retweet_count")), all = TRUE)

unique(df_full$screen_name)

df_full <- df_full %>%
  filter(screen_name != TRUE) %>%
  filter(screen_name != "screen_name")
 
unique(df_full$screen_name)

df_full <- df_full %>%
  rowwise() %>%
  mutate(text = cleaning_tweets(text)) %>%
  mutate(date = as.Date(created_at))

write.csv(df_full, "./data/clean/tweets_clean.csv", row.names = FALSE)
```
```{r}
tweets_clean <- read.csv("./data/clean/tweets_clean.csv")

join_info <- sheet %>%
  mutate(screen_name = gsub("@", "", Twitter.ID))

tweets_clean_joined <- left_join(tweets_clean, join_info, by = "screen_name")
tweets_clean_joined <- tweets_clean_joined %>%
  select(-c(Link, Follower, Notes, Twitter.ID))

write.csv(tweets_clean_joined, "./data/clean/tweets_clean_joined.csv", row.names = FALSE)
```


```{r}
tweets_clean_joined <- read.csv("./data/clean/tweets_clean_joined.csv")

tweets_clean_joined <- rename(tweets_clean_joined, name = Name, party = Party, country = Country, left_right = left.right)
corp_tweet <- corpus(tweets_clean_joined, text_field = "text")
print(corp_tweet)
```
```{r}
summary(corp_tweet, 5)
tok_tweet <- tokens(corp_tweet)
```

```{r}
tok_clean <- tokens_remove(tok_tweet, pattern = stopwords("german"))
o <- c("dass", "â€ž")
tok_clean <- tokens_select(tok_clean, pattern = o, selection = "remove")
View(tok_clean)
```



```{r}
tweet_matrix <- dfm(tok_clean)
tweet_matrix
dict <- read_tsv("./de.tsv")
head(dict)
```

```{r}
dict$word <- tolower(dict$word)
```

```{r}
#dict1 <- as.dictionary(dict)
#tw_mat_fin <- dfm_lookup(tweet_matrix, dict1)
tweet_matrix_df <- as.data.frame(tweet_matrix)
```
```{r}
# preprocessing = function(text){
#   # this function takes a text and returns the term-frequency vector of it
#   # as a data frame
#   
#   exclude = c("can", "i", "you", "one")  # list of stop words
#   
#   # cleaning and preprocessing
#   text = gsub(",", "", text)               
#   text = gsub("[[:punct:]]", "", text)
#   text = gsub("[[:digit:]]", "", text)
#   words = strsplit(text, " ")[[1]]           
#   words = tolower(words)  
#   
#    # term frequencies
#   tf = table(words)           
#   tf = as.data.frame(tf)
#   colnames(tf) = c("Word", "Frequency")
#   tf = subset(tf, is.element(Word, exclude) == FALSE)
#   
#   return(tf)
# }
# 
# net_valence_score = function(text, emo){
#   tf = preprocessing(text)
#   combined = merge(tf, emo, by="Word")
#   score = sum(combined$Frequency * combined$Valence) / sum(combined$Frequency)
#   return(score)
# }
# 
# 
# for(i in 1:nrow(reviews)){
#   text = texts[i]
#   score = net_valence_score(text, emo)
#   valence_scores = c(valence_scores, score)
# }
```



